###  特征工程(Feature Engineering)

1. **定义**:

   > 本质是一项工程活动, 目的是最大限度从原始数据中提取特征以供算法和模型使用。
   >
   > 特征 -----> 从数据中抽取出来对结果预测有用的信息

2. **重要性:**

   - 数据和特征决定一个机器学习的上限，而模型和算法只是逼近这个上限而已.

   - 特征越好,灵活性越强;

   - 特征越好,模型越简单;

   - 特征越好, 性能越出色;

3. **思维导图**

   ![](D:\work\accumulation\DataMining\data\特征工程.png)

   ![](D:\work\accumulation\DataMining\data\微信图片_20191226174136.png)

4. **数值特征(numerical feature)**

   > 可以是连续(continuous)的, 也可以是离散的(discrete), 一般表示为一个实数值
   > 例如: 年龄，价格， 身高， 体重， 测量数据.

   *不同的算法对于数值特征的处理要求不同*

5. **标准化(Standardization)**
   $$
   x^\prime = \frac{x - \mu}{\sigma} (\mu 为平均值, \sigma为标准差)
   $$
   特点: 通过对原始数据进行变化把数据变化到0，方差为1范围内

6. **归一化(Normalization)**

   - 应用场景: 多个特征同等重要的时候需要进行归一化处理

   - 目的:  使得某一个特征对最终结果不会造成更大的影响

   - 特点: 通过对原始数据进行变换吧数据映射到某一区间内(默认为[0, 1]) 之间

     >  注: 作用与每一列，$max(x)$  为一列的最大值， $min(x)$ 为一列的最小值，那么 $X^{\prime\prime}$ 为最终结果, $mx$, $mi$ 分别为制定区间默认值$mx$为1, $mi$ 为 0

$$
X^\prime = \frac{x- min(x)}{max(x) - min(x)}
$$

$$
X^{\prime\prime} = X^\prime *(mx - mi) + mi**标准化和归一化有何异同:**
$$
7. **标准化和归一化有何异同:**

   这俩都是线性变换,将原有数据进行了放缩,归一化一般放缩到[0,1],标准化则转为服从标准正态分布,数据的大小顺序没有发生改变 影响归一化的主要是两个极值,而标准化里每个数据都会影响计算均值和误差

   8. **多项式特征(Polynomial Features)**

      多项式特征可以理解为对现有特征的乘积, 比如现在有特征A, 特征B,  特征C, 那么就可以得到特征A 的平方 $A^2, A*B, A*C, B^2, B*C, 以及C^2$ 新生的这些变量即原有的变量的有机组合, 换句话, 当两个变量各自与 $y$ 的关系并不强的时候，把他们结合成为一个新的变量可能更容易体现出它们与$y$ 的关系  

   9. **据分箱技术(binning)特性**

        1. 定义

           > 数据分箱(也称为离散分箱或分段) 是一种数据预处理技术, 用于减少要观察误差的影响，是一种将多个连续值分组为较小数据的"分箱"的方法.

        2. 重要性以及优势
           > 一般在建立模型时，需要对连续变量离散化，特征离散化后，模型会更加稳定，降低了模型过拟合的风险

           - 离散特性的增加和减少都很容易，易于模型的快速迭代;
           - 稀疏向量内积乘法运算速度快，计算结果方便存储，容易拓展;
           - 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。4. 如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
           - 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单6. 独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
           - 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
           - 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问;
           - 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
           - 可以将缺失作为独立的一类带入模型。
           - 将所有变量变换到相似的尺度上。

        3. 方法:
           1. 有监督分箱

              - 卡方分箱:

                > 自底向上的(即基于合并的)数据离散化方法. 它依赖于卡方验证: 具有最小的卡方值的相邻区间合并在一起，直到满足确定的停止准则。

                *基本思想*:

                > 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。

                ![卡方分箱流程](/home/pawn/work/accumulation/数据挖掘笔记/data/卡方分箱流程.webp)

                **注意初始化时需要对实例进行排序， 在排序的基础上进行合并**

                - **卡方阀值的确定**:

                  根据显著性水平和自由度得到卡方值自由度比类别数量小1。例如：有3类,自由度为2，则90%置信度(10%显著性水平)下，卡方的值为4.6。

                - **阀值的意义:**

                  类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。 大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。

                  **注:**

                  1. ChiMerge算法推荐使用0.90、0.95、0.99置信度,最大区间数取10到15之间.
                  2. 也可以不考虑卡方阈值,此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限,最多几个区间,最少几个区间。
                  3. 对于类别型变量,需要分箱时需要按照某种方式进行排序。

              - 最小熵法分箱

                1. 假设因变量为分类变量， 可取值 $1, ... J_0$ 令 $P_{ij}$ 表示第 $i$ 个分箱内因变量取值为 $j$ 的观测的比例 $i=1, ... , k, j=1, ..., j;$ 那么第$i$ 个分箱的熵值为 $\sum_{j=o}^j-p_{ij} × logp_{ij}$ 如果第 $i$ 个分箱内因变量各类别的比例相等， 即$p_{11} =p_{12} = p_{1J} = 1/J $, 那么第$i$个分箱的熵值达到最大值； 如果第$i$ 个分箱内变量只有一种取值，即某个$P_{ij}$等于1而其他类别的比例等于0，那么第$i$个分箱的熵值达到最小值。
                2. 令 $r_i$表示第$i$个分箱的观测数占所有观测数的比例; 那么总熵值为 $\sum_{i=0}^k\sum_{j=0}^j(-p_{ij} × logp_{ij}$  需要使总熵值达到 最小， 也就是使分箱能够最大限度地区分因变量的各类类别

           2. **无监督分箱**

              - 等距分箱

                > 从最小值到最大值之间,均分为 N 等份, 这样, 如果 A,B 为最小最大值, 则每个区间的长度为$ W=(B−A)/N$ , 则区间边界值为$A+W,A+2W,….A+(N−1)W$ 。这里只考虑边界，每个等份里面的实例数量可能不等。

              - 等频率分箱

                > 区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。比如说 N=10 ,每个区间应该包含大约10%的实例。

              以上两种算法的弊端：

              ​	比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大

              - **聚类分箱**

                > 基于k均值聚类的分箱：
                >
                > k均值聚类法将观测值聚为k类，但在聚类过程中需要保证分箱的有序性：第一个分箱中所有观测值都要小于第二个分箱中的观测值，第二个分箱中所有观测值都要小于第三个分箱中的观测值，等等。

   10. **自适应分箱- 分位数切分**
         ​     

      > 自适应分箱是根据分布的特点，去划分区间。分位数分箱是常见的自适应分箱. 这种方法将一些分位数设为区间端点

   11. **对数 指数 变换 BOX-COX**

       > 对数变换是将变量映射为其对数的变换。如果将指数级分箱的分箱宽度设置为无限小，就成了对数变换。

       *对数变化是一个性能是能对大树枝进行压缩， 对小数值进行扩展。 所以， 重尾分布在经过对数变换后，会变得更均匀一些。这会给线性模型特征空间低端值争取 "呼吸空间"， 使模型不必咋输入变化很小的情况下去拟合变化非常大的目标值。*

       - 指数变换

         指数变换是一个变化族，是方差稳定变换，对数变换是其中的一种。这些变换可以推广为Box-Cox变换
         $$
         y^{(\lambda)} = \begin{cases}  \frac{y^\lambda- 1}{\lambda} ,& \text{$\lambda \not= 0$} \\ lny, & \text{$\lambda = 0$} \end{cases}
         $$

       - BOX—COX 变换
       Box-Cox 变换的作用是基于极大似然法的幂转换, 其所用是让分布在不丢失信息的前提下, 具有更好的性质（独立性、方差齐性、正态性等），以便得到更好的模型。λ无需计算，Scipy的boxcox函数，会找到最优λ，使变换后最近正态分布。 

### 特征工程—类别型(categorical feature)

1. 定义:

   > 类别型特征主要是制定的性别(男，女) 血型之类的 (A, B, AB, O) 只在有限选项内取值的特征。类别型特征原始输入通常为字符串类型，除了少数模型能直接处理字符串形式的输入，如逻辑回归， 支持向量机等模型来说， 类别型特征必须经过处理转换成数值型特征才能正确工作

2.分类：

- 序号编码

  序号编码通常用于处理类别间具有大小关系的数据

- 独热编码(one - hot)

  one-hot 编码用于处理类别间不具有大小关系的特征;

  但是当类别取值较多的时候需要注意:

  - 用稀疏向量来节省空间
  - 配合特征选择来降低维度

- 二进制编码

   二进制编码，先用序号编码给每个类别赋予一个类别id, 然后将类别id 转为二进制编码，

   本质上是利用二进制  对id 进行了hash 映射，得到 0-1 特征向量， 且维度小于 one-hot 编码。

3. 特征二值化

   > 定量特征二值化的核心在于设定一个阀值，大于阀值的赋值为1，小于阀值的赋值为0
$$
   x^\prime = \begin{cases} 1 ,& \text{$x > threshold $} \\ 0, & \text{$x \leq threshold $} \end{cases}
$$











