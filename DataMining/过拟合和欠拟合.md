# 过拟合和欠拟合
### 概念
- 过拟合
> 模型过于简单，未能充分捕获样本数据的特征。表现为模型在训练集上的效果不好
- 欠拟合
> 模型过于复杂，过分捕获样本数据的特征，从而将样本数据中一些特殊特征当成了共性特征。表现为模型在训练集上的效果非常好，但是在未知数据上的表现效果不好。

### 解决方案
如果产生欠拟合，可以采用如下的方式，来达到更好的拟合效果。
- 增加迭代次数。
- 增加模型复杂度。
   - 引入新的特征(多项式扩展).
- 使用更复杂的模型
   - 由线性模型改为非线性模型。

如果产生过拟合，可以采用如下方式， 来降低拟合的程度。

- 收集更多的数据
- 降低模型的复杂度
  - 使用正则化
- 减少迭代次数
- 选择简单的模型

### 3. 多项式扩展

#### 3.1 多项式扩展规则

**多项式扩展** 可以认为是对现有数据进行的一种转换，通过生成新的特征，从而额将数据由低维映射到高纬空间中，这样模型就可以拟合更广泛的数据。

我们将原始的特征称为**输入特征**， 将多项式扩展之后的特征称为**输出特征**，多项式扩展的规则如下(假设为n阶扩展):

- 对所有输入特征进行乘积组合
- 每个输入特征具有一个指数，指数的取值范围为$[0, n]$.
- 遍历所有指数可能的取值，但是需要保证所有输入特征的指数之和不超过 $n$ (小于等于n).
- 对每个符合上述条件的指数组合，作为扩展之后的输出特征。

假设，我们有如下的二元线性模型：

$$ \hat{y} =  w_0 + w_1x_1 + w_2x_2$$

如果该模型的拟合效果不佳，我们就介意对模型进行多项式扩展。例如, 我们进行2阶扩展(也可以进行更高阶的扩展)，符合条件的指数组合为:

$$ [x_1^0x_2^0， x_1^1x_2^0, x_1^0x_2^1, x_1^2x_2^0, x_1^1x_2^1, x_1^0x_2^2] $$

因此，经过多项式扩展后，最终的输出特征为:

$$[1, x_1, x_2, x_1^2, x_1x_2, x_2^2]$$

模型最终也变为:

$$\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2$$

当进行多项式扩展后，特征数量由之前的2个变成5个 (特征维度从2维变成了5维)，从而可以更灵活的去拟合数据。

经过多项式扩展后，我们依然可以使用之前的线性回归模型去拟合数据，这是因为，我们可以假设:

$$\vec{z} = (z_1,z_2,z_3,z_4,z_5) = (x_1,x_2, x_1x_2, x_1^2, x_2^2)$$

这样，之前的模型就会变成:

$$\vec{y} = w_0 + w_1z_1 + w_2z_2 + w_3z_3 + w_4z_4 + w_5z_5$$

从而, 我们依然可以认为，这还是一种线性模型.

### 3.2 PolynomailFeatures

我们可以使用 sklearn 中提供的 PolynomialFeatures 类来实现多项式扩展。通过powers_属性可以获取扩展之后每个输入特征的指数组合(矩阵)。 关于指数矩阵，描述如下:

- 矩阵的每一列对应输入特征，即列数等于输入特征数。
- 矩阵的每一行对应输出特征，即行数等于输出特征数。
- 矩阵的形状为 【输出特征数，输入特征数】
- 矩阵的值表示每个输入特征的指数值.
  - powers_[i, j] 表示第 i 个输入特征中，第j 个输入特征的指数值。

例如, 如果输入样本的特征数为2， 多项式扩展数为2，则指数矩阵为:

$$
powers = 
\begin{bmatrix} 
0 & 0 \\ 
1 & 0 \\
0 & 1 \\
2 & 0 \\
1 & 1 \\
0 & 2
\end{bmatrix}
$$
对于两个输入特征 $x_1$ 与 $x_2$ 来讲，多项式转换之后的值为:
$$[1, x_1, x_2, x_1^2, x_1x_2, x_2^2]$$
即:
$$[1, x_1, x_2, x_1^2, x_1x_2, x_2^2]$$
### 3.3 流水线
流水线(Pipeline类)可以将每个评估器视为一个步骤，然后将多个步骤作为一个整体而依次执行，这样，我们就无需分别执行每一步骤。就可以将多项式转换与训练模型两个步骤为一个整体，一并执行。
流水线具有最后一个评估器的所有方法. 当通过流水线对象调用方法 $f$时, 会执行这样的过程(假设流水线具有$n$个评估器):

- 如果f是fit或fit_transform 方法，则会首先对前面$n -1$ 个评估器依次调用fit_transform方法，然后在最后一个评估器上调用$f$方法。
- 如果$f$是其他方法，则会首先前$n -1$个评估器依次调用transform 方法，然后在最后一个评估器上调用$f$方法.
### 4. 常用的正则化
在线性回归中，模型过于复杂，通常表现为模型的参数过大(指绝对值过大)，即如果模型的参数过大，就容易出现过拟合现象。我们可以通过正则化来降低过拟合的程度。**正则化**，就是通过在损失函数中加入关于权重的惩罚项，进而限制模型的参数过大，从而减低过拟合，增加的惩罚项，我们称作正则项。

根据正则项的不同，我们可以将正则化分为如下几种:

- L2正则化
- L1正则化
- Elastic Net

#### 4.1  L2 正则化

L2正则化是最常用的正则化，将所有权重的平方和作为正则项。使用L2正则的线性回归模型称为Ridge回归(岭回归)。 加入L2 正则化的损失函数为:

$$
J(w) = \frac{1}{2}\sum_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2 + \alpha \sum_{j=1}^n w_j^2
$$

- $m$: 样本数量。
- $n$: 特征数量
- $\alpha$: 惩罚系数 $(a > 0)$

从包含正则项的损失函数中，我们可以发现，我们将损失函数分为两部分，一部分为原来的损失函数，另一部分为正则项的惩罚， 这样，如果当权重过大时，即使原来的损失函数值很小，但是整个项的损失值会很多， 因此， 整个损失值(二者的和)也不会很小， 从而，权重过大的$w$ 就可能不会称为最佳解。

### 4.2 L1正则化

L1 正则化使用所有权重的绝对值和作为正则项。使用L1正则的线性回归模型称为 Lasso 回归(Least Absolute Shrinkage and Selection Operator ---最小绝对收缩与选择因子)
$$
J(w) = \frac{1}{2}\sum_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2 + \alpha \sum_{j=1}^n |w_j|
$$

### 4.3  Elastic Net

Elastic Net(弹性网络)，同时将绝对值和与平方和作为正则项，是L1正则化与L2正则化之间的一个折中。使用该正则项的线性回归模型称为 Elastic Net 算法。
$$
J(w) =  \frac{1}{2}\sum_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2 + \alpha (\sum_{j=1}^n |w_j| + (1-p)\sum_{j=1}^nw_j^2)
$$

- $p:$L1 正则化的比重 ($0 <= p <= 1$)

### 4.4 正则化说明

- Lasson更容易产生稀疏矩阵，这就减少了模型所依赖的特征数量。因此，可以使用Lasso进行特征选择.
- Ridge模型具有较高的稳定性.
- Elastic Net是Lasso 与Ridge之间的一个折中，其可以像Lasso一样产生稀疏解，同时具有Ridge的部分稳定型.
- 当多个特征具有相关性，Lasso可能只会随机选择其中一个，而Elastic Net 可能会选择多个.