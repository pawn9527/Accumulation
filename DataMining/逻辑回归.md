# 逻辑回归
## 模型简介
> 逻辑回归是一个分类算法.

## 算法的分类思想
逻辑回归实现分类的思想为: 将每条样本进行“打分”，然后设置一个阀值，达到这个阀值，比较随意，划分为哪个类别都可以，但是，要保证阀值划分的一致性。
## 算法模型
对于逻辑回归，模型的前面与线性回归类似:
$$
\begin{eqnarray*}
z &=& w_1 + x_1 + w_2x_2 + ... ... + w_nx_n + b \\
  &=& \sum_{j=1}^n w_jx_j + b \\
  &=& \sum_{j=0}^n w_jx_j \\
  &=& \vec{w}^T * \vec{x}
\end{eqnarray*}
$$
不过，$z$的值是一个连续的值，取值范围为$(-\infty, +\infty)$, 我们可以将阀值设置为中间的位置，也就是0, 当 $z > 0$时, 模型将样本判定为一个类别(正例), 当$z \leq 0$ 时, 模型将样本判定为另外一个类别(负例)。 这样， 模型就实现了二分类的任务。

**说明: 在scikit-learn, 如果z值为0，则判定为负例**
## sigmoid函数
### 函数原型
对于分类任务来说，如果仅仅给分类的结果, 在某些场景下, 提供的信息可能并不充足，这就会带来一定的局限. 因此，我们建立分类模型，不仅应该能够进行分类，同时，也应该能够提供样本属于该类别的概率，在这现实中是非常实用的。例如, 某人患病的概率，明天下雨的概率等.

因此，我们需要将$z$ 的值转换为概率值，逻辑回归使用sigmoid 函数来实现转换，该函数的原型为:
$$
sigmoid(z) = \frac{1}{1 + e^{-z}}
$$
当$z$ 的值从$-\infty$向$+\infty$过渡时，sigmoid 函数的取值范围为$(0, 1)$ , 这个正好是概率的取值范围，当 z = 0时，sigmoid(0) 的值为 0.5，因此，模型就可以将sigmoid的输出作为正例的概率，而 $1-p$作为负列的概率。以阀值0.5作为两个分类的标准，就是看$p$ 与$1-p$那个类别的概率值更大，预测结果就是那个类别。

假设真实的分类$y$的值为1与0，则:
$$
\hat{y} = 
\begin{cases} 
1 &\text{$z > 0$}  \\
0 &\text{$z \leq 0$}  \\
\end{cases}
$$
当 $z=0$ 时，$sigmoid(z)$的值为0.5， 因此:
$$
\hat{y} = 
\begin{cases} 
1 &\text{$sigmoid(z) > 0.5$}  \\
0 &\text{$sigmoid(z) \leq 0.5$}  \\
\end{cases}
$$
转换成概率就是:
$$
\hat{y} =
\begin{cases} 
1 &\text{$p > 0.5$}  \\
0 &\text{$p \leq 0.5$}  \\
\end{cases}
$$

### sigmoid 函数图像

```python
import numpy as np
import matplotlib.pyplot as plt
# 定义sigmoid 函数.
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 200)
plt.plot(z, sigmoid(z))
# 绘制水平线与垂直线.
plt.axvline(x=0, ls='--', c='k')
plt.axhline(ls=':', c='k')
plt.axhline(y=0.5, ls=':', c='k')
plt.axhline(y=1, ls=':', c='k')
plt.xlabel('z值')
plt.ylabel('sigmoid(z)值')
```

## 逻辑回归损失函数

### 损失函数

根据之前的介绍，我们可以将类别$y (1与0)$ 的概率表示如下(这里使用s代表sigmoid函数):
$$
p(y=1|x;w) = s(z) \\
p(y=0|x;w) = 1 - s(z)
$$
我们可以将以上的两个式子综合表示为:
$$
p(y|x; w) = s(z)^y(1-s(z))^{1-y}
$$
以上是一个样本的概率，我们要求解能够使所有联合密度最大的$w$值， 因此，根据**极大似然估计**，所有样本的联合概率密度函数(即似然函数)为:
$$
\begin{eqnarray*}
L(w)  &=&  \prod_{i=1}^m  p(y^{(i)}|x^{(i)}; w) \\
 &=& \prod_{i=1}^m s(z^{(i)})^{y{(i)}}(1-s(z^{(i)}))^{1-y^{(i)}}
\end{eqnarray*}
$$
为了方便求解，我们取对数似然函数，让累计乘积变成累计求和:
$$
\begin{eqnarray*}
\ln{L(w)} &=& \ln{(\prod_{i=1}^m s(z^{(i)})^{y{(i)}}(1-s(z^{(i)}))^{1-y^{(i)}}} \\
&=& \sum_{i=1}^m(y^{(i)})\ln{sz^{(i)} + (1 - y^{(i)})\ln(1 - s(z^{(i)}))}
\end{eqnarray*}
$$
我们要使得上式的值最大，可以采用梯度上升的方式。不过，这里我们为了引入损失函数的概念， 我们采用相反的方式，即只需要使得该值的相反数最小即可， 因此，我们可以将上式的相反数作为逻辑回归的损失函数(对数损失函数):
$$
J(w) = -\sum_{i=1}^m(y^{(i)}\ln{s(z^{(i)})} + (1 - y^{(i)})\ln{(1 - s(z^{(i)}))})
$$

### 损失函数可视化

```python
s = np.linspace(0.01, 0.99, 200)
for y in [0, 1]:
    loss = -y * np.log(s) - (1 - y) * np.log(1 - s)
    plt.plot(s, loss, label=f"y={y}")
plt.legend()
plt.xlabel('sigmoid(z)')
plt.ylabel('J(w)')
plt.title("损失函数J(w)与sigmoid(z)的关系")
plt.show()
```

### 多分类实现细节

多分类在实现上，可以采用两种方式:

- one versus rest (一对其他)
- multinomial (多项式)