# 机器学习
[TOC]
### Supervised learning
#### Definition Term
- input features -- $x^{(i)}$
- target variable -- y^{(i)}

- training example -- (x^{(i)}, y^{(i)})

- training set 


$$
  {(x^{(i)}, y^{(i)});i=1,...,m}
$$



- hypothesis  ---- h(x)
- parameters ---- $\theta$

### Linear Regression

y as a linear function of x:
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2
$$
Here, the \theta_i are the parameters 

$x_0 = 1$ (this is the intercept term ), so that
$$
h(x) =  \sum_{i=0}^{n}\theta_ix_i = \theta^Tx,
$$
cost function:
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
$$

### LMS algorithm
We want to choose $\theta $  so as to minimize $J_(\theta)$,

The gradient descent algorithm
$$
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$
(This update is simultaneously performed for all values of j = 0, ... , n.)

Here, $\alpha$ is called the **learning rate**

So that we can neglect the sum in the definition of J. We have:
$$
\frac{\partial}{\partial\theta_j}J(\theta) = \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x) - y)^2 
= 2*\frac{1}{2}(h_\theta(x) -y)*\frac{\partial}{\partial\theta_j}(h_\theta(x) - y )
= (h_\theta -y)*\frac{\partial}{\partial\theta_j}(\sum_{i=0}^n\theta_ix_i -y) = (h_0(x) - y)
$$




For a single training example, this gives the update rule:
$$
\theta_j:=\theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}
$$
The rule is called the **LMS** update rule(least mean squares) 

and is also known as the **Widrow-Hoff** learning rule.



`Repeat until convergence`
$$
{
   \theta_j := \theta_j + \alpha\sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}  --(for every j)
}
$$

**m is training example number**

The reader can easily verify that the quantity in the summation in the update rule above is just
\frac{\partial{(\theta)}{\partial\theta_j}  (for the original definition of J)

So this is simply gradient descent on the original cost function J . This method looks aat every example

in the entire training set on every step, and is called

 **batch gradient descent**

for i=1, to,m 
$$
{
	 {
		\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)} 
		-- (for every j).
	}
}
$$




This algorithm is called **stochastic gradient descent **(also **incremental gradient des**)

### The normal equations

#### Matrix derivatives

For a funtion f : $R^{(mxn)}$ --> R mapping from  m- by -n



## Linear Regression with Multiple Variables(多变量线性回归)

#### 多维特征

x^{(i)} 代表第 i 个训练实例, 就是特征矩阵的第i行, 是一个**向量(vector)**
$$
x^{(2)} = \begin{vmatrix}
1416 \\ 3 \\ 2  \\ 40
\end{vmatrix}
$$
x_j^{(i)} 代表特征矩阵中的第i行的第j个特征, 也就是第i个训练实例的第j个特征

x_x^{(2)} = 3 , x_3^{(2)} =2 

支持多变量的假设h 表示为: $h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$

如果引入 x_0 = 1 

h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n

此时模型的参数是一个 n +1 维的向量, 任何一个训练实例也都是 n+1维的向量. 特征矩阵 x 的维度 $m*(n+1)$

因此公式可简化为: 
$$
h_\theta(x) = \theta^TX
$$
T 代表矩阵转置

#### 多变量梯度下降

代价函数:
$$
J(\theta_0, \theta_1, ..., \theta_n)= \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))^2
$$
其中:
$$
h_\theta(x) = \theta^TX = \theta_0 +\theta_1x_1 +\theta_2x_2 + ... + \theta_nx_n
$$
![1538402709180](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538402709180.png)

即:

![1538402731372](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538402731372.png)

求导后:

![1538402756252](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538402756252.png)



当n>=1时,
$$
\theta_0 := \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})x_0^{(i)}
$$

$$
\theta_1 := \theta_1 -  \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})x_1^{(i)}
$$

$$
\theta_1 := \theta_2 -  \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})x_2^{(i)}
$$

我们开始随机选择一系列参数值, 计算所有预测结构后, 再给所有的参数一个新的值, 如果循环到收敛:

$ J(\theta_0, \theta_1, ..., \theta_n)= \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))^2 $ 

之中:

$h_\theta(x) = \theta^TX = \theta_0 +\theta_1x_1 +\theta_2x_2 + ... + \theta_nx_n$

Python代码:

```python
def computeCost(X, y, theta):
	inner = np.power(((X * theta.T) ‐ y), 2) 
    return np.sum(inner) / (2 * len(X))
```

#### 特征缩放

最简单的方法就是:

![1538403640779](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538403640779.png)



尝试将所有的特征的尺度都尽量缩放到-1到1之间

![1538403719568](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538403719568.png)
$$
x_n  = \frac{x_n - u_n }{S_n}
$$
其中$u_n$ 是平均值, $S_n$是标准差

#### 学习率

![1538403735889](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538403735889.png)



梯度下降算法的每次迭代受到学习率的影响, 如果学习率$\alpha$ 过小

则达到收敛的迭代次数会非常高,如果学习率$\alpha$ 过大, 每次迭代可能不会减小代价函数, 可能会超过最小值导致无法收敛

通常可以考虑尝试这些学习率:
$$
x = 0.01, 0.03, 0.1, 0.3, 1, 3, 10
$$

#### 特征和多项式回归

比如房屋预测问题:

![1538403757093](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538403757093.png)

$ h_\theta(x) = \theta_0 + \theta_ 1 * frontage + \theta_2 * depth $

临街宽度纵向深度x_1 = frontage(临街宽度), x_2= depth(纵向深度) 

如果 面积  x = frontage * depth = ares(面积)

则: $h_\theta(x) = \theta_0 + \theta_1x$ 

线性回归并不适合所有的数据,  我们需要更多的曲线来适应我们的数据

比如二次模型:
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2
$$
或者是三次模型:
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2 + \theta_3x_3^3
$$
![1538404134664](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538404134664.png)

通常我们需要先观察数据然后再决定怎样的模型.

我们可以令:

$x_2  = x_2^2$, $x_3 = x_3^3$

从而将模型转化为线性回归模型

或者我们也可以根据图形:
$$
h_\theta(x) = \theta_0 + \theta_1(size) + \theta_1(size)^2
$$
或者:
$$
h_\theta(x) = \theta_0 + \theta_1(size) + \theta_1\sqrt{size}
$$
如果我们采用多项式回归模型, 在运行梯度下降算法前, 特征缩放非常有必要

#### 正规方程

到现在为止, 我们使用梯度下降算法,  但是对于有些线性回归问题, 正规方程是最好的解决办法, 如:

![1538404582087](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538404582087.png)

正规方程是通过求解方程来计算出代价函数最小的参数:
$$
\frac{\partial}{\partial\theta_j}J(\theta) = 0
$$
假设我们的训练集特征矩阵为 $X$(包含了$x_0=1$)

并且我们的训练结果为向量$y$

则利用方程解出向量
$$
\theta= (X^TX)^-1X^Ty
$$
上标T代表矩阵的转置, 上标 -1 代表矩阵的逆

设矩阵 $ A=X^TX $

则: $ (X^TX)^-1 = A^-1 $



![1538405141225](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538405141225.png)

即:

![1538405161751](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538405161751.png)

运用正规方程方法求解参数:

![1538405190637](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1538405190637.png)

在 Octave 中, 正规方程为:

```octave
prinv(x' * x)*x'*y
```

注: 对于不可逆矩阵不生效(非奇异矩阵)

| 梯度下降                    | 正规方程                                                     |
| --------------------------- | :----------------------------------------------------------- |
| 需要选择学习率 $\alpta$     | 不需要                                                       |
| 需要多次迭代                | 一次运算                                                     |
| 当特征数量$n$大时候比较适用 | 需要计算$(X^TX)^-1$ 如果特征数量$n$ 较大则运算代价大, 因为矩阵逆的运算复杂度为 $O(n^3)$通常n< 10000时候还是可以接受的 |
| 适用于各种类型的模型        | 只适用于线性模型,不适用于逻辑回归模型等其他模型              |

总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数     的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。

 

随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那  些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其     他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯    度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。



正规方程python实现

```python
import numpy as np
def normalEqn(X, y):
    theta = np.linalg,inv(x.T@X)@X.T@y # X.T@X = X.T.dot(X)
    return theta	
```

#### 正规方程及不可逆性(笔记本)