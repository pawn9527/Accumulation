###  特征工程(Feature Engineering)

1. **定义**:

   > 本质是一项工程活动, 目的是最大限度从原始数据中提取特征以供算法和模型使用。
   >
   > 特征 -----> 从数据中抽取出来对结果预测有用的信息

2. **重要性:**

   >数据和特征决定一个机器学习的上限，而模型和算法只是逼近这个上限而已.

   >特征越好,灵活性越强;
   >
   >特征越好,模型越简单;
   >
   >特征越好, 性能越出色;

3. **思维导图**

   ![](/home/pawn/work/accumulation/数据挖掘笔记/data/微信图片_20191226174136.png)

   ![](/home/pawn/work/accumulation/数据挖掘笔记/data/特征工程.png)

   4. **不同类型的数据的处理方法**

         1. **数值特征(numerical feature)**
   
            > 可以是连续(continuous)的, 也可以是离散的(discrete), 一般表示为一个实数值
         >
            > 例如: 年龄，价格， 身高， 体重， 测量数据.

            *不同的算法对于数值特征的处理要求不同*

         2. **标准化(Standardization)**

           > 特点: 通过对原始数据进行变化把数据变化到0，方差为1范围内
   
         $$
        x^\prime = \frac{x - \mu}{\sigma} (\mu 为平均值, \sigma为标准差)
        $$

         3. **归一化(Normalization)**

           应用场景: 多个特征同等重要的时候需要进行归一化处理

           目的:  使得某一个特征对最终结果不会造成更大的影响
   
           特点: 通过对原始数据进行变换吧数据映射到某一区间内(默认为[0, 1]) 之间
        $$
      X^\prime = \frac{x- min(x)}{max(x) - min(x)}
      $$
   
      $$
        X^{\prime\prime} = X^\prime *(mx - mi) + mi
      $$

       > 注: 作用与每一列，$max(x)$  为一列的最大值， $min(x)$ 为一列的最小值，那么 $X^{\prime\prime}$ 为最终结果, $mx$, $mi$ 分别为制定区间默认值$mx$为1, $mi$ 为 0

      **标准化和归一化有何异同:**

      > 这俩都是线性变换,将原有数据进行了放缩,归一化一般放缩到[0,1],标准化则转为服从标准正态分
      > 布,数据的大小顺序没有发生改变 影响归一化的主要是两个极值,而标准化里每个数据都会影响,因为
      > 计算均值和标准差。
   
   4. **多项式特征(Polynomial Features)**
  
      > 多项式特征可以理解为对现有特征的乘积, 比如现在有特征A, 特征B,  特征C, 那么就可以得到特征A 的平方 $A^2, A*B, A*C, B^2, B*C, 以及C^2$ 新生的这些变量即原有的变量的有机组合, 换句话, 当两个变量各自与 $y$ 的关系并不强的时候，把他们结合成为一个新的变量可能更容易体现出它们与$y$ 的关系  
  
   5. **据分箱技术(binning)特性**
  
      1. 定义
        
         > 数据分箱(也称为离散分箱或分段) 是一种数据预处理技术, 用于减少要观察误差的影响，是一种将多个连续值分组为较小数据的"分箱"的方法.
        
      2. 重要性以及优势
        
         > 一般在建立模型时，需要对连续变量离散化，特征离散化后，模型会更加稳定，降低了模型过拟合的风险
        
         - 离散特性的增加和减少都很容易，易于模型的快速迭代;
         - 稀疏向量内积乘法运算速度快，计算结果方便存储，容易拓展;
         - 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。4. 如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
         - 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单6. 独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
         - 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
         - 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问;
         - 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
         - 可以将缺失作为独立的一类带入模型。
         - 将所有变量变换到相似的尺度上。
        
      3. 方法:
        
         1. 有监督分箱
         
            - 卡方分箱:
            
              > 自底向上的(即基于合并的)数据离散化方法. 它依赖于卡方验证: 具有最小的卡方值的相邻区间合并在一起，直到满足确定的停止准则。
            
              *基本思想*:
            
              > 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。
            
              ![卡方分箱流程](/home/pawn/work/accumulation/数据挖掘笔记/data/卡方分箱流程.webp)
            
              **注意初始化时需要对实例进行排序， 在排序的基础上进行合并**
            
              - **卡方阀值的确定**:
            
                根据显著性水平和自由度得到卡方值自由度比类别数量小1。例如：有3类,自由度为2，则90%置信度(10%显著性水平)下，卡方的值为4.6。
              
              - **阀值的意义:**
              
                类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。 大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。
              
                **注:**
              
                1. ChiMerge算法推荐使用0.90、0.95、0.99置信度,最大区间数取10到15之间.
                2. 也可以不考虑卡方阈值,此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限,最多几个区间,最少几个区间。
                3. 对于类别型变量,需要分箱时需要按照某种方式进行排序。
              
            - 最小熵法分箱
         
              1. 假设因变量为分类变量， 可取值 $1, ... J_0$ 令 $P_{ij}$ 表示第 $i$ 个分箱内因变量取值为 $j$ 的观测的比例 $i=1, ... , k, j=1, ..., j;$ 那么第$i$ 个分箱的熵值为 $\sum_{j=o}^j-p_{ij} × logp_{ij}$ 如果第 $i$ 个分箱内因变量各类别的比例相等， 即$p_{11} =p_{12} = p_{1J} = 1/J $, 那么第$i$个分箱的熵值达到最大值； 如果第$i$ 个分箱内变量只有一种取值，即某个$P_{ij}$等于1而其他类别的比例等于0，那么第$i$个分箱的熵值达到最小值。
              2. 令 $r_i$表示第$i$个分箱的观测数占所有观测数的比例; 那么总熵值为 $\sum_{i=0}^k\sum_{j=0}^j(-p_{ij} × logp_{ij}$  需要使总熵值达到 最小， 也就是使分箱能够最大限度地区分因变量的各类类别
         
         2. **无监督分箱**
         
            - 等距分箱
         
              > 从最小值到最大值之间,均分为 N 等份, 这样, 如果 A,B 为最小最大值, 则每个区间的长度为$ W=(B−A)/N$ , 则区间边界值为$A+W,A+2W,….A+(N−1)W$ 。这里只考虑边界，每个等份里面的实例数量可能不等。
         
            - 等频率分箱
         
              > 区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。比如说 N=10 ,每个区间应该包含大约10%的实例。
         
            以上两种算法的弊端：
         
            ​		比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大
         
            - **聚类分箱**
         
              > 基于k均值聚类的分箱：
              >
              > k均值聚类法将观测值聚为k类，但在聚类过程中需要保证分箱的有序性：第一个分箱中所有观测值都要小于第二个分箱中的观测值，第二个分箱中所有观测值都要小于第三个分箱中的观测值，等等。
     
   6. **自适应分箱- 分位数切分**
     
      > 自适应分箱是根据分布的特点，去划分区间。分位数分箱是常见的自适应分箱. 这种方法将一些分位数设为区间端点
     
   7. **对数 指数 变换 BOX-COX**
      
      > 对数变换是将变量映射为其对数的变换。如果将指数级分箱的分箱宽度设置为无限小，就成了对数变换。
      
      *对数变化是一个性能是能对大树枝进行压缩， 对小数值进行扩展。 所以， 重尾分布在经过对数变换后，会变得更均匀一些。这会给线性模型特征空间低端值争取 "呼吸空间"， 使模型不必咋输入变化很小的情况下去拟合变化非常大的目标值。*
      - 指数变换
      > 指数变换是一个变化族，是方差稳定变换，对数变换是其中的一种。这些变换可以推广为Box-Cox变换
      $$
      y^{(\lambda)} = \begin{cases}  \frac{y^\lambda- 1}{\lambda} ,& \text{$\lambda \not= 0$} \\ lny, & \text{$\lambda = 0$} \end{cases}
      $$
      
      
      
      - BOX—COX 变换
      > Box-Cox 变换的作用是基于极大似然法的幂转换, 其所用是让分布在不丢失信息的前提下, 具有更好的性质（独立性、方差齐性、正态性等），以便得到更好的模型。λ无需计算，Scipy的boxcox函数，会找到最优λ，使变换后最近正态分布。

### 特征工程—类别型(categorical feature)

1. 定义:

   > 类别型特征主要是制定的性别(男，女) 血型之类的 (A, B, AB, O) 只在有限选项内取值的特征。类别型特征原始输入通常为字符串类型，除了少数模型能直接处理字符串形式的输入，如逻辑回归， 支持向量机等模型来说， 类别型特征必须经过处理转换成数值型特征才能正确工作

2.分类：

- 序号编码

  序号编码通常用于处理类别间具有大小关系的数据

- 独热编码(one - hot)

  one-hot 编码用于处理类别间不具有大小关系的特征;

  但是当类别取值较多的时候需要注意:

  - 用稀疏向量来节省空间
  - 配合特征选择来降低维度

- 二进制编码

   二进制编码，先用序号编码给每个类别赋予一个类别id, 然后将类别id 转为二进制编码，

   本质上是利用二进制  对id 进行了hash 映射，得到 0-1 特征向量， 且维度小于 one-hot 编码。

3. 特征二值化

   > 定量特征二值化的核心在于设定一个阀值，大于阀值的赋值为1，小于阀值的赋值为0 

   $$
   x^\prime = \begin{cases} 1 ,& \text{$x > threshold $} \\ 0, & \text{$x \leq threshold $} \end{cases}
   $$

   

